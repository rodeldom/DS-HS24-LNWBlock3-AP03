{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. OpenAI VLM (GPT-4*) - Basics\n",
    "This section demonstrates the basic usage of OpenAI's Vision Language Model (VLM) capabilities using GPT-4.\n",
    "We will use the OpenAI API to analyze an image and provide detailed textual insights.\n",
    "\n",
    "**Support Material**:\n",
    "- https://platform.openai.com/docs/guides/text-generation \n",
    "- https://platform.openai.com/docs/guides/vision?lang=node\n",
    "- https://platform.openai.com/docs/guides/text-generation?text-generation-quickstart-example=image \n",
    "- https://platform.openai.com/docs/api-reference/chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv  \n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "import textwrap\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "#openAIclient = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n",
    "openAIclient = openai.OpenAI(api_key= os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TEXTMODEL = \"gpt-4o-mini\" \n",
    "IMGMODEL= \"gpt-4o-mini\" \n",
    "\n",
    "# Path to your image\n",
    "img = \"images/street_scene.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"The image depicts a busy urban street scene with various characters and elements:\\n\\n- In\n",
      "the foreground, there's a young person sitting on the ground, focused on a device, while another figure appears to be\n",
      "lying nearby.\\n- A flower pot with blooms is positioned next to them.\\n- A bench with two people is visible; one is\n",
      "reading a newspaper while the other is looking at a book.\\n- In the background, several vehicles are moving along the\n",
      "street, and people are crossing the road.\\n- A cyclist and a person holding a guitar are also present in the scene,\n",
      "alongside some pigeons. \\n- The surrounding architecture features a mix of historic and modern buildings, contributing\n",
      "to the lively urban atmosphere. \\n\\nOverall, the scene captures the hustle and bustle of city life.\", refusal=None,\n",
      "role='assistant', audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "#basic call to gpt4 with prompt and image\n",
    "\n",
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
    "                        #\"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# Wrap the text to a specified width\n",
    "# Dies extrahiert die Antwort des Modells aus der API-Antwort. completion.choices[0].message gibt die erste Wahl (Antwort) des Modells zurück.\n",
    "# str(): Wandelt die Antwort in einen String um, damit sie weiterverarbeitet werden kann.\n",
    "response = str(completion.choices[0].message)\n",
    "print(textwrap.fill(response, width=120))  # --< Diese Funktion wird verwendet, um den Text umzubrechen, damit er leichter zu lesen ist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "messages=[...]: Dies ist die Eingabe, die an das Modell gesendet wird, und sie folgt dem Format eines Dialogs:\n",
    "role: Gibt die Rolle der Nachricht an. In diesem Fall ist der Benutzer die Quelle der Eingabe mit \"role\": \"user\".\n",
    "content: Dies sind die tatsächlichen Daten, die an das Modell geschickt werden. Es handelt sich um eine Liste von Nachrichten:\n",
    "Erste Nachricht: Ein einfacher Textprompt: \"What's in this image?\". Das Modell wird aufgefordert, den Inhalt des Bildes zu beschreiben.\n",
    "Zweite Nachricht: Ein Bild, das im Base64-Format eingebunden wird. Das Bild wird durch den Base64-String kodiert und als Bild-URL übergeben. Es wird innerhalb des image_url-Feldes übergeben.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1.1 Structured Output\n",
    "Here, we expand upon the VLM example to request structured outputs. This approach allows for extracting \n",
    "well-organized information from images in a machine-readable format, such as JSON.\n",
    "\n",
    "**Support Material**:\n",
    "- https://platform.openai.com/docs/guides/text-generation?text-generation-quickstart-example=json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def promptLLM(prompt : str = None, sysprompt : str = None,  image : str = None, wantJson : bool = False, returnDict : bool = False):\n",
    "    returnValue = \"\"\n",
    "    messages = [{\"role\": \"system\", \"content\" : sysprompt}]\n",
    "    modelToUse = TEXTMODEL\n",
    "    #force it to be a json answer prompt\n",
    "    #prompt = prompt if not wantJson else returnJSONAnswerPrompt(prompt)\n",
    "    messages.append({\"role\": \"user\", \"content\": [{ \n",
    "        \"type\" : \"text\", \n",
    "        \"text\" : prompt \n",
    "    }]})\n",
    "    if image is not None:\n",
    "        image = f\"data:image/jpeg;base64,{image}\"\n",
    "        messages[1][\"content\"].append({\"type\": \"image_url\", \"image_url\": { \"url\" : image}})\n",
    "        modelToUse = IMGMODEL\n",
    "\n",
    "    if wantJson:\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            #max_tokens= 400,\n",
    "            response_format={ \"type\": \"json_object\" },\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    else :\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    returnValue = returnValue.choices[0].message.content\n",
    "    if returnDict:\n",
    "        return json.loads(returnValue)\n",
    "    return returnValue\n",
    "\n",
    "\n",
    "\n",
    "# Die Funktion promptLLM() dient dazu, eine Anfrage (Prompt) an ein LLM (Large Language Model) zu senden und entweder eine einfache Antwort oder eine JSON-Antwort zurückzugeben. \n",
    "# Sie akzeptiert eine Reihe von Parametern und nutzt die OpenAI-API, um die Anfrage zu verarbeiten. \n",
    "# Hier ist eine Erklärung der einzelnen Teile und Parameter der Funktion:\n",
    "    # prompt (str, optional): Der Text, den der Benutzer an das Modell senden möchte, um eine Antwort zu erhalten. Wenn dieser nicht None ist, wird er als Benutzeranfrage an das Modell gesendet.\n",
    "    # sysprompt (str, optional): Ein System-Prompt, der dem Modell zusätzliche Anweisungen oder Kontext gibt. Dies könnte verwendet werden, um das Verhalten des Modells zu steuern (z.B. damit es eine detaillierte Antwort gibt oder in einem bestimmten Stil antwortet).\n",
    "    # image (str, optional): Ein Bild im Base64-codierten Format. Wenn dieses Bild angegeben wird, wird es zusammen mit dem Text-Prompt an das Modell gesendet. Das Bild wird in eine URL umgewandelt und als image_url in der Anfrage hinzugefügt.\n",
    "    # wantJson (bool, optional): Wenn True, wird die Antwort als JSON-Objekt zurückgegeben. Andernfalls wird die Antwort als normaler Text zurückgegeben.\n",
    "    # returnDict (bool, optional): Wenn True, wird die Antwort als Python-Wörterbuch (dict) zurückgegeben, nachdem sie aus der JSON-Antwort des Modells extrahiert wurde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funktionsweise:\n",
    "\n",
    "1. **Systemnachricht (sysprompt):**\n",
    "   - Es wird eine Nachricht mit der Rolle \"system\" erstellt, die Kontext oder Anweisungen für das Modell enthält.\n",
    "\n",
    "2. **Benutzernachricht (prompt):**\n",
    "   - Diese Nachricht wird unter der Rolle \"user\" an das Modell gesendet. Sie enthält den Text (und das Bild, falls vorhanden), den das Modell analysieren soll.\n",
    "\n",
    "3. **Bildverarbeitung:**\n",
    "   - Falls ein Bild angegeben wird, wird es in ein Base64-codiertes Format umgewandelt und als URL in die Anfrage eingefügt.\n",
    "\n",
    "4. **Modellauswahl:**\n",
    "   - Basierend auf dem Vorhandensein eines Bildes wird entweder **TEXTMODEL** oder **IMGMODEL** als Modell ausgewählt.\n",
    "   - **TEXTMODEL** wird verwendet, wenn nur Text verarbeitet wird, während **IMGMODEL** verwendet wird, wenn ein Bild mit gesendet wird.\n",
    "\n",
    "5. **API-Anfrage:**\n",
    "   - Die Anfrage wird an die OpenAI-API gesendet, wobei entweder ein normales Textformat oder ein JSON-Format angefordert wird.\n",
    "\n",
    "6. **Antwortverarbeitung:**\n",
    "   - Die Antwort des Modells wird verarbeitet und entweder als Text oder als JSON-Daten zurückgegeben.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funktionsaufruf (von oben)\n",
    "output = promptLLM(prompt = \"describe the image in detail\",sysprompt = \"you are a careful observer. the response should be in json format\", image = encode_image(img), wantJson=True, returnDict=True)\n",
    "\n",
    "\n",
    "# prompt: \"describe the image in detail\" – Dies ist der Text, der an das Modell gesendet wird, um eine detaillierte Beschreibung des Bildes zu erhalten. Der Benutzer fordert das Modell auf, die Szene im Bild genau zu beschreiben.\n",
    "\n",
    "# sysprompt: \"you are a careful observer. the response should be in json format\" – Hierbei handelt es sich um den System-Prompt. Er gibt dem Modell den Hinweis, dass es eine sehr sorgfältige Beobachtung der Szene durchführen soll und dass die Antwort im JSON-Format sein soll.\n",
    "\n",
    "# image: encode_image(img) – Dies ist das Bild, das codiert wurde, um es in einem Format zu übermitteln, das die API verstehen kann. encode_image(img) gibt die Base64-codierte Version des Bildes zurück.\n",
    "\n",
    "# wantJson: True – Dies gibt an, dass die Antwort im JSON-Format gewünscht wird.\n",
    "\n",
    "# returnDict: True – Dies bedeutet, dass die Antwort als Python-Wörterbuch (Dictionary) zurückgegeben wird, sodass du die Daten weiterverarbeiten kannst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': {'setting': 'A bustling urban street scene in a city, showcasing a mix of modern and historical architecture.',\n",
       "  'foreground': {'elements': [{'type': 'bench',\n",
       "     'details': {'material': 'wood',\n",
       "      'position': 'sidewalk',\n",
       "      'occupants': [{'gender': 'female',\n",
       "        'appearance': {'hair': 'short, blonde',\n",
       "         'clothing': 'red top, blue jeans',\n",
       "         'activity': 'reading a magazine'}},\n",
       "       {'gender': 'male',\n",
       "        'appearance': {'hair': 'gray',\n",
       "         'clothing': 'suit',\n",
       "         'activity': 'reading a newspaper'}}]}},\n",
       "    {'type': 'child',\n",
       "     'details': {'gender': 'male',\n",
       "      'appearance': {'hair': 'short, brown',\n",
       "       'clothing': 'green jacket, shorts',\n",
       "       'activity': 'using a smartphone',\n",
       "       'position': 'sitting on the ground'}}},\n",
       "    {'type': 'person',\n",
       "     'details': {'gender': 'male',\n",
       "      'appearance': {'clothing': 'red hoodie',\n",
       "       'position': 'lying on the ground'}}},\n",
       "    {'type': 'pigeons',\n",
       "     'details': {'quantity': 'multiple',\n",
       "      'activity': 'foraging on the ground'}},\n",
       "    {'type': 'flower pot',\n",
       "     'details': {'flowers': 'geraniums', 'position': 'next to the child'}}]},\n",
       "  'background': {'elements': [{'type': 'traffic',\n",
       "     'details': {'vehicles': [{'type': 'car',\n",
       "        'color': 'silver',\n",
       "        'activity': 'driving'},\n",
       "       {'type': 'taxi', 'activity': 'stopped'},\n",
       "       {'type': 'motorcycle', 'activity': 'riding'},\n",
       "       {'type': 'scooter', 'activity': 'riding'}]}},\n",
       "    {'type': 'pedestrians',\n",
       "     'details': {'quantity': 'multiple', 'activity': 'walking'}},\n",
       "    {'type': 'traffic light',\n",
       "     'details': {'state': 'red', 'position': 'above the crosswalk'}},\n",
       "    {'type': 'buildings',\n",
       "     'details': {'style': 'mix of modern and historical',\n",
       "      'notable features': 'large windows, storefronts'}}]},\n",
       "  'atmosphere': {'lighting': 'soft, warm light suggesting early evening',\n",
       "   'mood': 'busy yet relaxed'}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'elements': [{'type': 'bench',\n",
       "   'details': {'material': 'wood',\n",
       "    'position': 'sidewalk',\n",
       "    'occupants': [{'gender': 'female',\n",
       "      'appearance': {'hair': 'short, blonde',\n",
       "       'clothing': 'red top, blue jeans',\n",
       "       'activity': 'reading a magazine'}},\n",
       "     {'gender': 'male',\n",
       "      'appearance': {'hair': 'gray',\n",
       "       'clothing': 'suit',\n",
       "       'activity': 'reading a newspaper'}}]}},\n",
       "  {'type': 'child',\n",
       "   'details': {'gender': 'male',\n",
       "    'appearance': {'hair': 'short, brown',\n",
       "     'clothing': 'green jacket, shorts',\n",
       "     'activity': 'using a smartphone',\n",
       "     'position': 'sitting on the ground'}}},\n",
       "  {'type': 'person',\n",
       "   'details': {'gender': 'male',\n",
       "    'appearance': {'clothing': 'red hoodie',\n",
       "     'position': 'lying on the ground'}}},\n",
       "  {'type': 'pigeons',\n",
       "   'details': {'quantity': 'multiple', 'activity': 'foraging on the ground'}},\n",
       "  {'type': 'flower pot',\n",
       "   'details': {'flowers': 'geraniums', 'position': 'next to the child'}}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"description\"][\"foreground\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# JSON Schema for Controlled Structured Outputs\n",
    "In this section, we define a JSON schema for a more controlled and specific output from the model. \n",
    "Using this schema, we can ensure the model adheres to predefined data types and structures while describing images.In this case we will provide an exmaple of json format answer, but ideally \n",
    "one could also do it via e.g. pydantic library.\n",
    "\n",
    "Example: \n",
    "```\n",
    "from typing import List, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    position: str = Field(..., description=\"Position of the person in the environment, e.g., standing, sitting, etc.\")\n",
    "    age: int = Field(..., ge=0, description=\"Age of the person, must be a non-negative integer.\")\n",
    "    activity: str = Field(..., description=\"Activity the person is engaged in, e.g., reading, talking, etc.\")\n",
    "    gender: Literal[\"male\", \"female\", \"non-binary\", \"other\", \"prefer not to say\"] = Field(\n",
    "        ..., description=\"Gender of the person\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ImageExtraction(BaseModel):\n",
    "    number_of_people: int = Field(..., ge=0, description=\"The total number of people in the environment.\")\n",
    "    atmosphere: str = Field(..., description=\"Description of the atmosphere, e.g., calm, lively, etc.\")\n",
    "    hour_of_the_day: int = Field(..., ge=0, le=23, description=\"The hour of the day in 24-hour format.\")\n",
    "    people: List[Person] = Field(..., description=\"List of people and their details.\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Änderungen in der `promptLLM`-Methode\n",
    "\n",
    "1. **JSON-Schema für die Ausgabe:**  \n",
    "   - Einführung eines JSON-Schemas zur Strukturierung der Modellantwort (z.B. `numberOfPeople`, `atmosphere`, `hourOfTheDay`, `people`).\n",
    "\n",
    "2. **Strukturierte Antwort:**  \n",
    "   - Modellantwort muss vordefinierte Felder und Typen wie `age`, `activity`, `gender` enthalten.\n",
    "\n",
    "3. **Flexible Ausgabe:**  \n",
    "   - `wantJson` steuert, ob die Antwort als Text oder JSON zurückgegeben wird.\n",
    "   - Möglichkeit, die Antwort als Dictionary zu erhalten (`returnDict=True`).\n",
    "\n",
    "4. **Beispiel-JSON-Schema:**  \n",
    "   - Antwort muss Felder wie `numberOfPeople` und `people` mit spezifischen Datentypen enthalten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verbesserte Version der vorherigen promptLLM-Methode\n",
    "def promptLLM(prompt : str = None, sysprompt : str = None,  image : str = None, wantJson : bool = False, returnDict : bool = False):\n",
    "    returnValue = \"\"\n",
    "    messages = [{\"role\": \"system\", \"content\" : sysprompt}]\n",
    "    modelToUse = TEXTMODEL\n",
    "    #force it to be a json answer prompt\n",
    "    #prompt = prompt if not wantJson else returnJSONAnswerPrompt(prompt)\n",
    "    messages.append({\"role\": \"user\", \"content\": [{ \n",
    "        \"type\" : \"text\", \n",
    "        \"text\" : prompt \n",
    "    }]})\n",
    "    if image is not None:\n",
    "        image = f\"data:image/jpeg;base64,{image}\"\n",
    "        messages[1][\"content\"].append({\"type\": \"image_url\", \"image_url\": { \"url\" : image}})\n",
    "        modelToUse = IMGMODEL\n",
    "\n",
    "    if wantJson:\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            #max_tokens= 400,\n",
    "            response_format={\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"img_extract\",\n",
    "                    \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"numberOfPeople\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The total number of people in the environment\",\n",
    "                        \"minimum\": 0\n",
    "                        },\n",
    "                        \"atmosphere\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Description of the atmosphere, e.g., calm, lively, etc.\"\n",
    "                        },\n",
    "                        \"hourOfTheDay\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The hour of the day in 24-hour format\",\n",
    "                        \"minimum\": 0,\n",
    "                        \"maximum\": 23\n",
    "                        },\n",
    "                        \"people\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"description\": \"List of people and their details\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                            \"position\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Position of the person in the environment, e.g., standing, sitting, etc.\"\n",
    "                            },\n",
    "                            \"age\": {\n",
    "                                \"type\": \"integer\",\n",
    "                                \"description\": \"Age of the person\",\n",
    "                                \"minimum\": 0\n",
    "                            },\n",
    "                            \"activity\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Activity the person is engaged in, e.g., reading, talking, etc.\"\n",
    "                            },\n",
    "                            \"gender\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Gender of the person\",\n",
    "                                \"enum\": [\"male\", \"female\", \"non-binary\", \"other\", \"prefer not to say\"]\n",
    "                            }\n",
    "                            },\n",
    "                            \"required\": [\"position\", \"age\", \"activity\", \"gender\"]\n",
    "                        }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"numberOfPeople\", \"atmosphere\", \"hourOfTheDay\", \"people\"]\n",
    "                    }}},\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    else :\n",
    "        returnValue = openAIclient.chat.completions.create(\n",
    "            model=modelToUse,\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            #n=1,\n",
    "        )\n",
    "    returnValue = returnValue.choices[0].message.content\n",
    "    if returnDict:\n",
    "        return json.loads(returnValue)\n",
    "    return returnValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image_analysis = promptLLM(prompt = \"describe the image in detail\",sysprompt = \"you are a careful observer. the response should be in json format\", image = encode_image(img), wantJson=True, returnDict=True)\n",
    "\n",
    "# Das resultierende JSON gibt eine detaillierte Beschreibung des Bildes zurück, einschließlich der Anzahl der Personen, ihrer Aktivitäten und der allgemeinen Atmosphäre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alert service prompt  --> das Modell darauf vorzubereiten, eine Analyse der Szene in Bezug auf potenziell gefährdete Personen zu liefern\n",
    "                        #  potenziell gefährliche Situationen zu erkenne\n",
    "\n",
    "alert_sys_prompt = \" you are an experienced first aid paramedical\"\n",
    "alert_prompt= \"\"\"Extract from the following scene analysis give to you in json format, \n",
    "if anyone might be in danger and if the Child Hospital or normal Hospital should be alerted. \n",
    "Give the a concise answer\n",
    "The situation is given to you from this object: \"\"\" + str(output_image_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In this scene, the 15-year-old male who is lying down and appears to be unconscious or resting may be in danger. It is advisable to alert a normal hospital for potential medical assistance, as the situation does not specifically indicate a pediatric emergency that would require a Child Hospital.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promptLLM(prompt = alert_prompt, sysprompt= alert_sys_prompt) \n",
    "\n",
    "\n",
    "#  alert_prompt: Dies ist der eigentliche Text (Prompt), der beschreibt, was das Modell tun soll. In diesem Fall fordert der Prompt das Modell auf, potenziell gefährliche Situationen aus einer JSON-Darstellung einer Bildanalyse zu extrahieren und gegebenenfalls eine Empfehlung zu geben, ob das Krankenhaus oder ein Kinderkrankenhaus benachrichtigt werden sollte.\n",
    "\n",
    "# alert_sys_prompt: Dies ist der System-Prompt, der dem Modell eine Rolle zuweist, um die Aufgabe auszuführen. In diesem Fall wird das Modell als ein erfahrener paramedizinischer Notfallsanitäter eingesetzt, der in der Lage ist, medizinische Notfälle korrekt zu erkennen und zu bewerten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided image analysis, there is no mention of a 16-year-old individual. The closest ages mentioned are 15 and 20. Since there is no specific data for a 16-year-old, I cannot provide coordinates for that age group.\\n\\nIf you need assistance with a different age group or any other information, please let me know!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promptLLM(prompt = \"Considering the image analysis given\" +str(output_image_analysis)+ \"give me back the coordinates of the 16-years old. If these are not available, infer them form the pic\", sysprompt= alert_sys_prompt) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verlangt vom Modell, dass es die Bildanalyse berücksichtigt und die Koordinaten eines 16-jährigen Individuums zurückgibt. Falls keine expliziten Daten für eine 16-jährige Person vorliegen, soll das Modell versuchen, die Koordinaten basierend auf dem Bild zu schätzen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Antwort des Modells lautet:\n",
    "\n",
    "\"Basierend auf der bereitgestellten Bildanalyse gibt es keine Erwähnung eines 16-jährigen Individuums. Die nächstgelegenen Altersangaben sind 15 und 20. Da keine spezifischen Daten für einen 16-Jährigen vorliegen, kann ich keine Koordinaten für diese Altersgruppe bereitstellen.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[400, 600, 500, 700]'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promptLLM(prompt =  \"Detect if there is a person who is under 18 years old on the floor and reutrn its coordinates as a list in the format '[ymin,xmin, ymax, xmax]'. Just output the list.\", sysprompt= alert_sys_prompt, image = encode_image(img)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Google VLM (Gemini)\n",
    "This section demonstrates the use of Google's Vision Language Model, Gemini. \n",
    "We explore basic text generation as well as its ability to analyze images and provide relevant outputs.\n",
    "\n",
    "**Support Material**:\n",
    "- https://colab.research.google.com/drive/1eDvf_Ky9jLOZFShgHrm4GI-wkAaQnue6?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "from dotenv import load_dotenv  \n",
    "import google.generativeai as genai\n",
    "from PIL import Image\n",
    "\n",
    "load_dotenv()\n",
    "#genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# pip install google-generativeai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intelligence (AI) is a broad field encompassing various techniques that enable computers to mimic human intelligence.  There's no single \"how it works\" answer, as different AI approaches utilize different methods. However, we can break down some core concepts:\n",
      "\n",
      "**1. Data is King:**  AI systems learn from data.  The more relevant and high-quality data you feed an AI, the better it performs. This data can be anything from images and text to sensor readings and financial transactions.\n",
      "\n",
      "**2. Algorithms are the Engine:** Algorithms are sets of rules and statistical techniques that AI systems use to process data and learn patterns.  These algorithms are designed to:\n",
      "\n",
      "* **Identify patterns:** Find relationships and correlations within the data.\n",
      "* **Make predictions:** Use learned patterns to forecast future outcomes or classify new data.\n",
      "* **Learn and adapt:** Modify their behavior based on new data and feedback.\n",
      "\n",
      "**3. Key AI Techniques:**  Several prominent techniques drive AI systems:\n",
      "\n",
      "* **Machine Learning (ML):**  This involves training algorithms on large datasets so they can learn to perform tasks without explicit programming.  Instead of being explicitly programmed with rules, they learn from examples.  Examples include:\n",
      "    * **Supervised Learning:** The algorithm is trained on labeled data (e.g., images labeled \"cat\" or \"dog\").\n",
      "    * **Unsupervised Learning:** The algorithm is trained on unlabeled data and tries to find structure and patterns on its own (e.g., clustering similar customers).\n",
      "    * **Reinforcement Learning:** The algorithm learns through trial and error, receiving rewards or penalties for its actions (e.g., training a robot to walk).\n",
      "\n",
      "* **Deep Learning (DL):** A subset of machine learning that uses artificial neural networks with multiple layers (hence \"deep\"). These networks are inspired by the structure and function of the human brain and are particularly effective at processing complex data like images, speech, and text.  Examples include image recognition, natural language processing, and self-driving cars.\n",
      "\n",
      "* **Natural Language Processing (NLP):**  Focuses on enabling computers to understand, interpret, and generate human language.  This is used in chatbots, language translation, and sentiment analysis.\n",
      "\n",
      "* **Computer Vision:**  Allows computers to \"see\" and interpret images and videos.  This is used in facial recognition, object detection, and medical image analysis.\n",
      "\n",
      "**4. The Process (Simplified):**\n",
      "\n",
      "1. **Data Collection:** Gather and prepare relevant data.\n",
      "2. **Data Cleaning and Preprocessing:** Clean and format the data to be suitable for the algorithm.\n",
      "3. **Algorithm Selection:** Choose the appropriate algorithm based on the problem and data.\n",
      "4. **Training:** Feed the data to the algorithm to allow it to learn patterns.\n",
      "5. **Evaluation:** Test the algorithm's performance on new, unseen data.\n",
      "6. **Deployment:** Integrate the trained algorithm into a system to perform real-world tasks.\n",
      "7. **Monitoring and Improvement:** Continuously monitor the system's performance and refine the model as needed.\n",
      "\n",
      "\n",
      "**In essence:** AI systems learn to perform tasks by analyzing large amounts of data using sophisticated algorithms.  The specific techniques used depend on the problem being solved, but the core principle remains the same:  learning from data to make intelligent decisions or predictions.  It's important to note that despite advancements, AI systems are still tools; their capabilities are limited by the data they are trained on and the algorithms they employ.  They don't possess genuine consciousness or understanding in the human sense.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "response = model.generate_content(\"Explain how AI works\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup:\n",
    "- `genai.GenerativeModel(\"gemini-1.5-flash\")` initializes the Gemini model (version 1.5) for generating content.\n",
    "- The `generate_content` method is used to send a prompt to the model, in this case, asking for an explanation of AI.\n",
    "\n",
    "### Response:\n",
    "- The model processes the input prompt and generates a response based on its training, returning the output in `response.text`.\n",
    "\n",
    "### Output:\n",
    "- The text generated would then provide an explanation of how AI works, which may describe AI's ability to learn from data, use algorithms, and make predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[699, 326, 959, 627]\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(img)\n",
    "\n",
    "genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n",
    "model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
    "\n",
    "response = model.generate_content([\n",
    "    im,\n",
    "    (\n",
    "        \"Detect if there is a person who is under 18 years old on the floor and reutrn its coordinates as a list in the format '[ymin,xmin, ymax, xmax]'. Just output the list.\\n \"\n",
    "    ),\n",
    "])\n",
    "response.resolve()\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakdown of Code for Image Analysis Using Google Gemini Model\n",
    "\n",
    "1. **Image Loading**:\n",
    "   The image is loaded using the Python Imaging Library (PIL) with the function `Image.open(img)`. This reads the image and prepares it for processing by the model.\n",
    "\n",
    "2. **Model Configuration**:\n",
    "   The Gemini model is configured with an API key from the environment variable `GEMINI_API_KEY`, ensuring secure access to the model's services.\n",
    "\n",
    "3. **Model Initialization**:\n",
    "   The code uses the `gemini-1.5-pro` version, which is optimized for handling complex tasks like image analysis and object detection. This model is capable of generating content based on the prompts provided.\n",
    "\n",
    "4. **Prompt Setup**:\n",
    "   The `generate_content()` method is called, sending both the image and a prompt to the model. The prompt specifically instructs the model to detect if there is a person under 18 years old lying on the floor. It is asked to return the coordinates of the detected individual in the format `[ymin, xmin, ymax, xmax]`.\n",
    "\n",
    "5. **Response Handling**:\n",
    "   After the model processes the input, the `response.resolve()` method ensures that the generated content is correctly processed. The result is then displayed with `response.text`, which contains the coordinates of the detected person, if any.\n",
    "\n",
    "This process can be used to automate the identification of specific individuals in images, particularly useful for tasks such as monitoring or emergency response scenarios.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini can be used to predict bounding boxes based on free form text queries.\n",
    "The model can be prompted to return the boxes in a variety of different formats (dictionary, list, etc). This of course migh need to be parsed. \n",
    "Check: https://colab.research.google.com/drive/1eDvf_Ky9jLOZFShgHrm4GI-wkAaQnue6?usp=sharing#scrollTo=WFLDgSztv77H\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
